
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>metrics_eval package &#8212; metrics_eval 0.0.1 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="metrics_eval" href="modules.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="metrics-eval-package">
<h1>metrics_eval package<a class="headerlink" href="#metrics-eval-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-metrics_eval.ranking_metrics">
<span id="metrics-eval-ranking-metrics-module"></span><h2>metrics_eval.ranking_metrics module<a class="headerlink" href="#module-metrics_eval.ranking_metrics" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="metrics_eval.ranking_metrics.average_precision">
<code class="sig-prename descclassname">metrics_eval.ranking_metrics.</code><code class="sig-name descname">average_precision</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.ranking_metrics.average_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute average precision.</p>
<p>Average precision is a measure that combines recall and precision for ranked retrieval results. For one information need, the average precision is the mean of the precision scores after each relevant document is retrieved.</p>
<div class="math notranslate nohighlight">
\[AP = {1\over R}{{\sum\limits _{r}P &#64; r}}\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R\)</span> is the number of relevant documents;</p></li>
<li><p><span class="math notranslate nohighlight">\(r\)</span> values are the positions of relevant documents;</p></li>
<li><p><span class="math notranslate nohighlight">\(P &#64; r\)</span> is the <span class="math notranslate nohighlight">\(Precision\,at\,r\)</span>.</p></li>
</ul>
<div class="highlight-bibtex notranslate"><div class="highlight"><pre><span></span><span class="nc">@Inbook</span><span class="p">{</span><span class="nl">Zhang2009</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">&quot;Zhang, Ethan and Zhang, Yi&quot;</span><span class="p">,</span>
    <span class="na">editor</span><span class="p">=</span><span class="s">&quot;LIU, LING and {&quot;O}ZSU, M. TAMER&quot;</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">&quot;Average Precision&quot;</span><span class="p">,</span>
    <span class="na">bookTitle</span><span class="p">=</span><span class="s">&quot;Encyclopedia of Database Systems&quot;</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">&quot;2009&quot;</span><span class="p">,</span>
    <span class="na">publisher</span><span class="p">=</span><span class="s">&quot;Springer US&quot;</span><span class="p">,</span>
    <span class="na">address</span><span class="p">=</span><span class="s">&quot;Boston, MA&quot;</span><span class="p">,</span>
    <span class="na">pages</span><span class="p">=</span><span class="s">&quot;192--193&quot;</span><span class="p">,</span>
    <span class="na">isbn</span><span class="p">=</span><span class="s">&quot;978-0-387-39940-9&quot;</span><span class="p">,</span>
    <span class="na">doi</span><span class="p">=</span><span class="s">&quot;10.1007/978-0-387-39940-9_482&quot;</span><span class="p">,</span>
    <span class="na">url</span><span class="p">=</span><span class="s">&quot;https://doi.org/10.1007/978-0-387-39940-9_482&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents sorted by descending rank order.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Average Precision score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.ranking_metrics.binary_metrics">
<code class="sig-prename descclassname">metrics_eval.ranking_metrics.</code><code class="sig-name descname">binary_metrics</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.ranking_metrics.binary_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Hits at k, Precision at k, Recall at k, R-precision, Mean Reciprocal Rank, Mean Average Precision.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents sorted by descending rank order.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Mean Reciprocal Rank score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.ranking_metrics.dcg">
<code class="sig-prename descclassname">metrics_eval.ranking_metrics.</code><code class="sig-name descname">dcg</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.ranking_metrics.dcg" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Discounted Cumulative Gain (DCG) at k.</p>
<p>Discounted Cumulative Gain measures the usefulness, or _gain_, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks.</p>
<p>Standard formulation:</p>
<div class="math notranslate nohighlight">
\[DCG(k) = \sum_{n=1}^{k}\frac{2^{r_n}-1}{\log_2(n+1)}\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k\)</span> is the number of results to consider;</p></li>
<li><p><span class="math notranslate nohighlight">\(r_n\)</span> is the relevance of the item in position <span class="math notranslate nohighlight">\(n\)</span> of the result list;</p></li>
<li><p><span class="math notranslate nohighlight">\(\log_2(n)\)</span> is the _discount_ factor.</p></li>
</ul>
<div class="highlight-bibtex notranslate"><div class="highlight"><pre><span></span><span class="nc">@article</span><span class="p">{</span><span class="nl">jarvelin2002cumulated</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">&quot;Cumulated gain-based evaluation of IR techniques&quot;</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">&quot;J{\&quot;a}rvelin, Kalervo and Kek{\&quot;a}l{\&quot;a}inen, Jaana&quot;</span><span class="p">,</span>
    <span class="na">journal</span><span class="p">=</span><span class="s">&quot;ACM Transactions on Information Systems (TOIS)&quot;</span><span class="p">,</span>
    <span class="na">volume</span><span class="p">=</span><span class="s">&quot;20&quot;</span><span class="p">,</span>
    <span class="na">number</span><span class="p">=</span><span class="s">&quot;4&quot;</span><span class="p">,</span>
    <span class="na">pages</span><span class="p">=</span><span class="s">&quot;422--446&quot;</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">&quot;2002&quot;</span><span class="p">,</span>
    <span class="na">publisher</span><span class="p">=</span><span class="s">&quot;ACM&quot;</span><span class="p">,</span>
    <span class="na">doi</span><span class="p">=</span><span class="s">&quot;10.1145/582415.582418&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>If y_true and y_pred are multi-dimensional, it computes the arithmetic mean of the Discounted Cumulative Gain scores.</p>
<p>Example:
&gt;&gt;&gt; y_true = np.array([[[12, 0.5], [25, 0.3]], [[11, 0.4], [2, 0.6]]])
&gt;&gt;&gt; y_pred = np.array([[12, 234, 25, 36, 32, 35], [12, 11, 25, 36, 2, 35]])
&gt;&gt;&gt; k = 5
&gt;&gt;&gt; dcg(y_true, y_pred, k)
array([0.52978577, 0.40109345])</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs and true relevance scores of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Discounted Cumulative Gain at k score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.ranking_metrics.hit_list_at_k">
<code class="sig-prename descclassname">metrics_eval.ranking_metrics.</code><code class="sig-name descname">hit_list_at_k</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.ranking_metrics.hit_list_at_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute hit list at k.</p>
<p>Hit list at k is a list of k elements where each element is 1 if the corresponding element in y_pred is relevant, 0 otherwise.</p>
<p>Example:
&gt;&gt;&gt; y_true = np.array([1, 4, 5, 6])
&gt;&gt;&gt; y_pred = np.array([1, 2, 3, 4, 5, 7])
&gt;&gt;&gt; k = 5
&gt;&gt;&gt; hit_list_at_k(y_true, y_pred, k)
[1, 0, 0, 1, 1]</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Hit list at k.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Numpy Array or List of Numpy Arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.ranking_metrics.hits_at_k">
<code class="sig-prename descclassname">metrics_eval.ranking_metrics.</code><code class="sig-name descname">hits_at_k</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.ranking_metrics.hits_at_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute hits at k.</p>
<p>Hits at k is the number of relevant documents in the first k positions of the retrieved documents list.</p>
<p>If y_true and y_pred are bi-dimensional, it computes the arithmetic mean of the hits at k scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Hits at k score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.ranking_metrics.idcg">
<code class="sig-prename descclassname">metrics_eval.ranking_metrics.</code><code class="sig-name descname">idcg</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">k</span></em>, <em class="sig-param"><span class="n">binary</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.ranking_metrics.idcg" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Ideal Discounted Cumulative Gain (IDCG) at k.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List</em>) – (List of) Numpy Array of IDs of _relevant_ documents.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
<li><p><strong>binary</strong> (<em>Boolean</em>) – If <cite>True</cite> it will not reorder y_true by relevance score.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Discounted Cumulative Gain at k score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.ranking_metrics.map">
<code class="sig-prename descclassname">metrics_eval.ranking_metrics.</code><code class="sig-name descname">map</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.ranking_metrics.map" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute mean average precision.</p>
<p>The Mean Average Precision (MAP) is the arithmetic mean of the average precision values for a set of ranked lists.</p>
<div class="math notranslate nohighlight">
\[MAP = {1\over n}\sum\limits_n {AP_n }\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of tasks (ranked lists);</p></li>
<li><p><span class="math notranslate nohighlight">\(AP_n\)</span> is the <span class="math notranslate nohighlight">\(Average\,Precision\)</span> of <span class="math notranslate nohighlight">\(n\)</span>-th task.</p></li>
</ul>
<div class="highlight-bibtex notranslate"><div class="highlight"><pre><span></span><span class="nc">@Inbook</span><span class="p">{</span><span class="nl">Zhang2009</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">&quot;Zhang, Ethan and Zhang, Yi&quot;</span><span class="p">,</span>
    <span class="na">editor</span><span class="p">=</span><span class="s">&quot;LIU, LING and {\&quot;O}ZSU, M. TAMER&quot;</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">&quot;Average Precision&quot;</span><span class="p">,</span>
    <span class="na">bookTitle</span><span class="p">=</span><span class="s">&quot;Encyclopedia of Database Systems&quot;</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">&quot;2009&quot;</span><span class="p">,</span>
    <span class="na">publisher</span><span class="p">=</span><span class="s">&quot;Springer US&quot;</span><span class="p">,</span>
    <span class="na">address</span><span class="p">=</span><span class="s">&quot;Boston, MA&quot;</span><span class="p">,</span>
    <span class="na">pages</span><span class="p">=</span><span class="s">&quot;192--193&quot;</span><span class="p">,</span>
    <span class="na">isbn</span><span class="p">=</span><span class="s">&quot;978-0-387-39940-9&quot;</span><span class="p">,</span>
    <span class="na">doi</span><span class="p">=</span><span class="s">&quot;10.1007/978-0-387-39940-9_482&quot;</span><span class="p">,</span>
    <span class="na">url</span><span class="p">=</span><span class="s">&quot;https://doi.org/10.1007/978-0-387-39940-9_482&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents sorted by descending rank order.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Mean Average Precision score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.ranking_metrics.mrr">
<code class="sig-prename descclassname">metrics_eval.ranking_metrics.</code><code class="sig-name descname">mrr</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.ranking_metrics.mrr" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Mean Reciprocal Rank.</p>
<p>The mean reciprocal rank is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer: 1 for first place, ​1/2 for second place, 1/3 for third place and so on. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries.</p>
<p>Usefull when only one document is the correct answer.</p>
<div class="math notranslate nohighlight">
\[MRR = \frac{1}{N}\sum_{i=1}^{N}\frac{1}{rank_i}\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the number of tasks (ranked lists);</p></li>
<li><p><span class="math notranslate nohighlight">\(rank_i\)</span> is the position of the correct document for the task <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents sorted by descending rank order.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Mean Reciprocal Rank score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.ranking_metrics.ndcg">
<code class="sig-prename descclassname">metrics_eval.ranking_metrics.</code><code class="sig-name descname">ndcg</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span></em>, <em class="sig-param"><span class="n">binary</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.ranking_metrics.ndcg" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Normalized Discounted Cumulative Gain (NDCG) at k.</p>
<div class="math notranslate nohighlight">
\[nDCG(k) = \frac{DCG(k)}{IDCG(k)}\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(DCG(k)\)</span> is Discounted Cumulative Gain at k;</p></li>
<li><p><span class="math notranslate nohighlight">\(IDCG(k)\)</span> is Ideal Discounted Cumulative Gain at k (max possibile DCG at k).</p></li>
</ul>
<div class="highlight-bibtex notranslate"><div class="highlight"><pre><span></span><span class="nc">@article</span><span class="p">{</span><span class="nl">jarvelin2002cumulated</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">&quot;Cumulated gain-based evaluation of IR techniques&quot;</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">&quot;J{\&quot;a}rvelin, Kalervo and Kek{\&quot;a}l{\&quot;a}inen, Jaana&quot;</span><span class="p">,</span>
    <span class="na">journal</span><span class="p">=</span><span class="s">&quot;ACM Transactions on Information Systems (TOIS)&quot;</span><span class="p">,</span>
    <span class="na">volume</span><span class="p">=</span><span class="s">&quot;20&quot;</span><span class="p">,</span>
    <span class="na">number</span><span class="p">=</span><span class="s">&quot;4&quot;</span><span class="p">,</span>
    <span class="na">pages</span><span class="p">=</span><span class="s">&quot;422--446&quot;</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">&quot;2002&quot;</span><span class="p">,</span>
    <span class="na">publisher</span><span class="p">=</span><span class="s">&quot;ACM&quot;</span><span class="p">,</span>
    <span class="na">doi</span><span class="p">=</span><span class="s">&quot;10.1145/582415.582418&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>If y_true and y_pred are multi-dimensional, it computes the arithmetic mean of the Normalized Discounted Cumulative Gain scores.</p>
<p>Example:
&gt;&gt;&gt; y_true = np.array([[[12, 0.5], [25, 0.3]], [[11, 0.4], [2, 0.6]]])
&gt;&gt;&gt; y_pred = np.array([[12, 234, 25, 36, 32, 35], [12, 11, 25, 36, 2, 35]])
&gt;&gt;&gt; k = 5
&gt;&gt;&gt; ndcg(y_true, y_pred, k)
0.7525653965843032</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs and true relevance scores of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
<li><p><strong>binary</strong> (<em>Boolean</em>) – If <cite>True</cite> it will not reorder y_true by relevance score during iDCG computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Normalized Discounted Cumulative Gain at k score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.ranking_metrics.precision_at_k">
<code class="sig-prename descclassname">metrics_eval.ranking_metrics.</code><code class="sig-name descname">precision_at_k</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.ranking_metrics.precision_at_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute precision at k.</p>
<p>Precision at k (P&#64;k) is the proportion of the top-k documents that are relevant. The top-k documents are the first k documents of a ranked list.</p>
<div class="math notranslate nohighlight">
\[P&#64;k={{r}\over{k}}\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(r\)</span> is the number of relevant documents.</p></li>
</ul>
<p>If y_true and y_pred are bi-dimensional, it computes the arithmetic mean of the precision at k scores.</p>
<div class="math notranslate nohighlight">
\[mP&#64;k = {1\over n}\sum\limits_n {P&#64;k_n }\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of tasks;</p></li>
<li><p><span class="math notranslate nohighlight">\(P&#64;k_n\)</span> is the <span class="math notranslate nohighlight">\(Precision\,at\,k\)</span> of <span class="math notranslate nohighlight">\(n\)</span>-th task.</p></li>
</ul>
<div class="highlight-bibtex notranslate"><div class="highlight"><pre><span></span><span class="nc">@Inbook</span><span class="p">{</span><span class="nl">Craswell2009</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">&quot;Craswell, Nick&quot;</span><span class="p">,</span>
    <span class="na">editor</span><span class="p">=</span><span class="s">&quot;LIU, LING</span>
<span class="s">    and {\&quot;O}ZSU, M. TAMER&quot;</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">&quot;Precision at n&quot;</span><span class="p">,</span>
    <span class="na">bookTitle</span><span class="p">=</span><span class="s">&quot;Encyclopedia of Database Systems&quot;</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">&quot;2009&quot;</span><span class="p">,</span>
    <span class="na">publisher</span><span class="p">=</span><span class="s">&quot;Springer US&quot;</span><span class="p">,</span>
    <span class="na">address</span><span class="p">=</span><span class="s">&quot;Boston, MA&quot;</span><span class="p">,</span>
    <span class="na">pages</span><span class="p">=</span><span class="s">&quot;2127--2128&quot;</span><span class="p">,</span>
    <span class="na">isbn</span><span class="p">=</span><span class="s">&quot;978-0-387-39940-9&quot;</span><span class="p">,</span>
    <span class="na">doi</span><span class="p">=</span><span class="s">&quot;10.1007/978-0-387-39940-9_484&quot;</span><span class="p">,</span>
    <span class="na">url</span><span class="p">=</span><span class="s">&quot;https://doi.org/10.1007/978-0-387-39940-9_484&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents sorted by descending rank order.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Precision at k score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.ranking_metrics.r_precision">
<code class="sig-prename descclassname">metrics_eval.ranking_metrics.</code><code class="sig-name descname">r_precision</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.ranking_metrics.r_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute R-precision.</p>
<p>For a given query topic <span class="math notranslate nohighlight">\(Q\)</span>, R-precision is the precision at <span class="math notranslate nohighlight">\(R\)</span>, where <span class="math notranslate nohighlight">\(R\)</span> is the number of relevant documents for <span class="math notranslate nohighlight">\(Q\)</span>. In other words, if there are <span class="math notranslate nohighlight">\(r\)</span> relevant documents among the top-<span class="math notranslate nohighlight">\(R\)</span> retrieved documents, then R-precision is</p>
<div class="math notranslate nohighlight">
\[\frac{r}{R}\]</div>
<p>If y_true and y_pred are bi-dimensional, it computes the arithmetic mean of the R-precision scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents sorted by descending rank order.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>R-precision score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>FLoat</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.ranking_metrics.recall_at_k">
<code class="sig-prename descclassname">metrics_eval.ranking_metrics.</code><code class="sig-name descname">recall_at_k</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.ranking_metrics.recall_at_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute recall at k.</p>
<p>Recall at k (P&#64;k) is the ratio between the top-k documents that are relevant and the total number of relevant documents. The top-k documents are the first k in a ranking.</p>
<div class="math notranslate nohighlight">
\[R&#64;k={{r}\over{R}}\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(r\)</span> is the number of retrieved relevant documents at k.</p></li>
<li><p><span class="math notranslate nohighlight">\(R\)</span> is the total number of relevant documents.</p></li>
</ul>
<p>If y_true and y_pred are bi-dimensional, it computes the arithmetic mean of the recall at k scores.</p>
<div class="math notranslate nohighlight">
\[mR&#64;k = {1\over n}\sum\limits_n {R&#64;k_n }\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of tasks;</p></li>
<li><p><span class="math notranslate nohighlight">\(R&#64;k_n\)</span> is the <span class="math notranslate nohighlight">\(Recall\,at\,k\)</span> of <span class="math notranslate nohighlight">\(n\)</span>-th task.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents sorted by descending rank order.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Recall at k score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-metrics_eval.utils">
<span id="metrics-eval-utils-module"></span><h2>metrics_eval.utils module<a class="headerlink" href="#module-metrics_eval.utils" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="metrics_eval.utils.convert_trec_y_pred">
<code class="sig-prename descclassname">metrics_eval.utils.</code><code class="sig-name descname">convert_trec_y_pred</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.utils.convert_trec_y_pred" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="metrics_eval.utils.convert_trec_y_true">
<code class="sig-prename descclassname">metrics_eval.utils.</code><code class="sig-name descname">convert_trec_y_true</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.utils.convert_trec_y_true" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="metrics_eval.utils.remove_non_digits">
<code class="sig-prename descclassname">metrics_eval.utils.</code><code class="sig-name descname">remove_non_digits</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">s</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.utils.remove_non_digits" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="metrics_eval.utils.to_typed_list">
<code class="sig-prename descclassname">metrics_eval.utils.</code><code class="sig-name descname">to_typed_list</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ls</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.utils.to_typed_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a list of Numpy Arrays into a Numba Typed List.</p>
</dd></dl>

</div>
<div class="section" id="module-metrics_eval">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-metrics_eval" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="metrics_eval.hit_list_at_k">
<code class="sig-prename descclassname">metrics_eval.</code><code class="sig-name descname">hit_list_at_k</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.hit_list_at_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute hit list at k.</p>
<p>Hit list at k is a list of k elements where each element is 1 if the corresponding element in y_pred is relevant, 0 otherwise.</p>
<p>Example:
&gt;&gt;&gt; y_true = np.array([1, 4, 5, 6])
&gt;&gt;&gt; y_pred = np.array([1, 2, 3, 4, 5, 7])
&gt;&gt;&gt; k = 5
&gt;&gt;&gt; hit_list_at_k(y_true, y_pred, k)
[1, 0, 0, 1, 1]</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Hit list at k.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Numpy Array or List of Numpy Arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.hits_at_k">
<code class="sig-prename descclassname">metrics_eval.</code><code class="sig-name descname">hits_at_k</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.hits_at_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute hits at k.</p>
<p>Hits at k is the number of relevant documents in the first k positions of the retrieved documents list.</p>
<p>If y_true and y_pred are bi-dimensional, it computes the arithmetic mean of the hits at k scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Hits at k score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.precision_at_k">
<code class="sig-prename descclassname">metrics_eval.</code><code class="sig-name descname">precision_at_k</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.precision_at_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute precision at k.</p>
<p>Precision at k (P&#64;k) is the proportion of the top-k documents that are relevant. The top-k documents are the first k documents of a ranked list.</p>
<div class="math notranslate nohighlight">
\[P&#64;k={{r}\over{k}}\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(r\)</span> is the number of relevant documents.</p></li>
</ul>
<p>If y_true and y_pred are bi-dimensional, it computes the arithmetic mean of the precision at k scores.</p>
<div class="math notranslate nohighlight">
\[mP&#64;k = {1\over n}\sum\limits_n {P&#64;k_n }\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of tasks;</p></li>
<li><p><span class="math notranslate nohighlight">\(P&#64;k_n\)</span> is the <span class="math notranslate nohighlight">\(Precision\,at\,k\)</span> of <span class="math notranslate nohighlight">\(n\)</span>-th task.</p></li>
</ul>
<div class="highlight-bibtex notranslate"><div class="highlight"><pre><span></span><span class="nc">@Inbook</span><span class="p">{</span><span class="nl">Craswell2009</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">&quot;Craswell, Nick&quot;</span><span class="p">,</span>
    <span class="na">editor</span><span class="p">=</span><span class="s">&quot;LIU, LING</span>
<span class="s">    and {\&quot;O}ZSU, M. TAMER&quot;</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">&quot;Precision at n&quot;</span><span class="p">,</span>
    <span class="na">bookTitle</span><span class="p">=</span><span class="s">&quot;Encyclopedia of Database Systems&quot;</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">&quot;2009&quot;</span><span class="p">,</span>
    <span class="na">publisher</span><span class="p">=</span><span class="s">&quot;Springer US&quot;</span><span class="p">,</span>
    <span class="na">address</span><span class="p">=</span><span class="s">&quot;Boston, MA&quot;</span><span class="p">,</span>
    <span class="na">pages</span><span class="p">=</span><span class="s">&quot;2127--2128&quot;</span><span class="p">,</span>
    <span class="na">isbn</span><span class="p">=</span><span class="s">&quot;978-0-387-39940-9&quot;</span><span class="p">,</span>
    <span class="na">doi</span><span class="p">=</span><span class="s">&quot;10.1007/978-0-387-39940-9_484&quot;</span><span class="p">,</span>
    <span class="na">url</span><span class="p">=</span><span class="s">&quot;https://doi.org/10.1007/978-0-387-39940-9_484&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents sorted by descending rank order.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Precision at k score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.recall_at_k">
<code class="sig-prename descclassname">metrics_eval.</code><code class="sig-name descname">recall_at_k</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.recall_at_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute recall at k.</p>
<p>Recall at k (P&#64;k) is the ratio between the top-k documents that are relevant and the total number of relevant documents. The top-k documents are the first k in a ranking.</p>
<div class="math notranslate nohighlight">
\[R&#64;k={{r}\over{R}}\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(r\)</span> is the number of retrieved relevant documents at k.</p></li>
<li><p><span class="math notranslate nohighlight">\(R\)</span> is the total number of relevant documents.</p></li>
</ul>
<p>If y_true and y_pred are bi-dimensional, it computes the arithmetic mean of the recall at k scores.</p>
<div class="math notranslate nohighlight">
\[mR&#64;k = {1\over n}\sum\limits_n {R&#64;k_n }\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of tasks;</p></li>
<li><p><span class="math notranslate nohighlight">\(R&#64;k_n\)</span> is the <span class="math notranslate nohighlight">\(Recall\,at\,k\)</span> of <span class="math notranslate nohighlight">\(n\)</span>-th task.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents sorted by descending rank order.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Recall at k score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.r_precision">
<code class="sig-prename descclassname">metrics_eval.</code><code class="sig-name descname">r_precision</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.r_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute R-precision.</p>
<p>For a given query topic <span class="math notranslate nohighlight">\(Q\)</span>, R-precision is the precision at <span class="math notranslate nohighlight">\(R\)</span>, where <span class="math notranslate nohighlight">\(R\)</span> is the number of relevant documents for <span class="math notranslate nohighlight">\(Q\)</span>. In other words, if there are <span class="math notranslate nohighlight">\(r\)</span> relevant documents among the top-<span class="math notranslate nohighlight">\(R\)</span> retrieved documents, then R-precision is</p>
<div class="math notranslate nohighlight">
\[\frac{r}{R}\]</div>
<p>If y_true and y_pred are bi-dimensional, it computes the arithmetic mean of the R-precision scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents sorted by descending rank order.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>R-precision score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>FLoat</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.mrr">
<code class="sig-prename descclassname">metrics_eval.</code><code class="sig-name descname">mrr</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.mrr" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Mean Reciprocal Rank.</p>
<p>The mean reciprocal rank is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer: 1 for first place, ​1/2 for second place, 1/3 for third place and so on. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries.</p>
<p>Usefull when only one document is the correct answer.</p>
<div class="math notranslate nohighlight">
\[MRR = \frac{1}{N}\sum_{i=1}^{N}\frac{1}{rank_i}\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the number of tasks (ranked lists);</p></li>
<li><p><span class="math notranslate nohighlight">\(rank_i\)</span> is the position of the correct document for the task <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents sorted by descending rank order.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Mean Reciprocal Rank score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.average_precision">
<code class="sig-prename descclassname">metrics_eval.</code><code class="sig-name descname">average_precision</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.average_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute average precision.</p>
<p>Average precision is a measure that combines recall and precision for ranked retrieval results. For one information need, the average precision is the mean of the precision scores after each relevant document is retrieved.</p>
<div class="math notranslate nohighlight">
\[AP = {1\over R}{{\sum\limits _{r}P &#64; r}}\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R\)</span> is the number of relevant documents;</p></li>
<li><p><span class="math notranslate nohighlight">\(r\)</span> values are the positions of relevant documents;</p></li>
<li><p><span class="math notranslate nohighlight">\(P &#64; r\)</span> is the <span class="math notranslate nohighlight">\(Precision\,at\,r\)</span>.</p></li>
</ul>
<div class="highlight-bibtex notranslate"><div class="highlight"><pre><span></span><span class="nc">@Inbook</span><span class="p">{</span><span class="nl">Zhang2009</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">&quot;Zhang, Ethan and Zhang, Yi&quot;</span><span class="p">,</span>
    <span class="na">editor</span><span class="p">=</span><span class="s">&quot;LIU, LING and {&quot;O}ZSU, M. TAMER&quot;</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">&quot;Average Precision&quot;</span><span class="p">,</span>
    <span class="na">bookTitle</span><span class="p">=</span><span class="s">&quot;Encyclopedia of Database Systems&quot;</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">&quot;2009&quot;</span><span class="p">,</span>
    <span class="na">publisher</span><span class="p">=</span><span class="s">&quot;Springer US&quot;</span><span class="p">,</span>
    <span class="na">address</span><span class="p">=</span><span class="s">&quot;Boston, MA&quot;</span><span class="p">,</span>
    <span class="na">pages</span><span class="p">=</span><span class="s">&quot;192--193&quot;</span><span class="p">,</span>
    <span class="na">isbn</span><span class="p">=</span><span class="s">&quot;978-0-387-39940-9&quot;</span><span class="p">,</span>
    <span class="na">doi</span><span class="p">=</span><span class="s">&quot;10.1007/978-0-387-39940-9_482&quot;</span><span class="p">,</span>
    <span class="na">url</span><span class="p">=</span><span class="s">&quot;https://doi.org/10.1007/978-0-387-39940-9_482&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents sorted by descending rank order.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Average Precision score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.map">
<code class="sig-prename descclassname">metrics_eval.</code><code class="sig-name descname">map</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.map" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute mean average precision.</p>
<p>The Mean Average Precision (MAP) is the arithmetic mean of the average precision values for a set of ranked lists.</p>
<div class="math notranslate nohighlight">
\[MAP = {1\over n}\sum\limits_n {AP_n }\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of tasks (ranked lists);</p></li>
<li><p><span class="math notranslate nohighlight">\(AP_n\)</span> is the <span class="math notranslate nohighlight">\(Average\,Precision\)</span> of <span class="math notranslate nohighlight">\(n\)</span>-th task.</p></li>
</ul>
<div class="highlight-bibtex notranslate"><div class="highlight"><pre><span></span><span class="nc">@Inbook</span><span class="p">{</span><span class="nl">Zhang2009</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">&quot;Zhang, Ethan and Zhang, Yi&quot;</span><span class="p">,</span>
    <span class="na">editor</span><span class="p">=</span><span class="s">&quot;LIU, LING and {\&quot;O}ZSU, M. TAMER&quot;</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">&quot;Average Precision&quot;</span><span class="p">,</span>
    <span class="na">bookTitle</span><span class="p">=</span><span class="s">&quot;Encyclopedia of Database Systems&quot;</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">&quot;2009&quot;</span><span class="p">,</span>
    <span class="na">publisher</span><span class="p">=</span><span class="s">&quot;Springer US&quot;</span><span class="p">,</span>
    <span class="na">address</span><span class="p">=</span><span class="s">&quot;Boston, MA&quot;</span><span class="p">,</span>
    <span class="na">pages</span><span class="p">=</span><span class="s">&quot;192--193&quot;</span><span class="p">,</span>
    <span class="na">isbn</span><span class="p">=</span><span class="s">&quot;978-0-387-39940-9&quot;</span><span class="p">,</span>
    <span class="na">doi</span><span class="p">=</span><span class="s">&quot;10.1007/978-0-387-39940-9_482&quot;</span><span class="p">,</span>
    <span class="na">url</span><span class="p">=</span><span class="s">&quot;https://doi.org/10.1007/978-0-387-39940-9_482&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents sorted by descending rank order.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Mean Average Precision score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.binary_metrics">
<code class="sig-prename descclassname">metrics_eval.</code><code class="sig-name descname">binary_metrics</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.binary_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Hits at k, Precision at k, Recall at k, R-precision, Mean Reciprocal Rank, Mean Average Precision.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents sorted by descending rank order.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Mean Reciprocal Rank score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.dcg">
<code class="sig-prename descclassname">metrics_eval.</code><code class="sig-name descname">dcg</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.dcg" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Discounted Cumulative Gain (DCG) at k.</p>
<p>Discounted Cumulative Gain measures the usefulness, or _gain_, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks.</p>
<p>Standard formulation:</p>
<div class="math notranslate nohighlight">
\[DCG(k) = \sum_{n=1}^{k}\frac{2^{r_n}-1}{\log_2(n+1)}\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k\)</span> is the number of results to consider;</p></li>
<li><p><span class="math notranslate nohighlight">\(r_n\)</span> is the relevance of the item in position <span class="math notranslate nohighlight">\(n\)</span> of the result list;</p></li>
<li><p><span class="math notranslate nohighlight">\(\log_2(n)\)</span> is the _discount_ factor.</p></li>
</ul>
<div class="highlight-bibtex notranslate"><div class="highlight"><pre><span></span><span class="nc">@article</span><span class="p">{</span><span class="nl">jarvelin2002cumulated</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">&quot;Cumulated gain-based evaluation of IR techniques&quot;</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">&quot;J{\&quot;a}rvelin, Kalervo and Kek{\&quot;a}l{\&quot;a}inen, Jaana&quot;</span><span class="p">,</span>
    <span class="na">journal</span><span class="p">=</span><span class="s">&quot;ACM Transactions on Information Systems (TOIS)&quot;</span><span class="p">,</span>
    <span class="na">volume</span><span class="p">=</span><span class="s">&quot;20&quot;</span><span class="p">,</span>
    <span class="na">number</span><span class="p">=</span><span class="s">&quot;4&quot;</span><span class="p">,</span>
    <span class="na">pages</span><span class="p">=</span><span class="s">&quot;422--446&quot;</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">&quot;2002&quot;</span><span class="p">,</span>
    <span class="na">publisher</span><span class="p">=</span><span class="s">&quot;ACM&quot;</span><span class="p">,</span>
    <span class="na">doi</span><span class="p">=</span><span class="s">&quot;10.1145/582415.582418&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>If y_true and y_pred are multi-dimensional, it computes the arithmetic mean of the Discounted Cumulative Gain scores.</p>
<p>Example:
&gt;&gt;&gt; y_true = np.array([[[12, 0.5], [25, 0.3]], [[11, 0.4], [2, 0.6]]])
&gt;&gt;&gt; y_pred = np.array([[12, 234, 25, 36, 32, 35], [12, 11, 25, 36, 2, 35]])
&gt;&gt;&gt; k = 5
&gt;&gt;&gt; dcg(y_true, y_pred, k)
array([0.52978577, 0.40109345])</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs and true relevance scores of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Discounted Cumulative Gain at k score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.idcg">
<code class="sig-prename descclassname">metrics_eval.</code><code class="sig-name descname">idcg</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">k</span></em>, <em class="sig-param"><span class="n">binary</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.idcg" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Ideal Discounted Cumulative Gain (IDCG) at k.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List</em>) – (List of) Numpy Array of IDs of _relevant_ documents.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
<li><p><strong>binary</strong> (<em>Boolean</em>) – If <cite>True</cite> it will not reorder y_true by relevance score.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Discounted Cumulative Gain at k score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="metrics_eval.ndcg">
<code class="sig-prename descclassname">metrics_eval.</code><code class="sig-name descname">ndcg</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="n">k</span></em>, <em class="sig-param"><span class="n">binary</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_eval.ndcg" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Normalized Discounted Cumulative Gain (NDCG) at k.</p>
<div class="math notranslate nohighlight">
\[nDCG(k) = \frac{DCG(k)}{IDCG(k)}\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(DCG(k)\)</span> is Discounted Cumulative Gain at k;</p></li>
<li><p><span class="math notranslate nohighlight">\(IDCG(k)\)</span> is Ideal Discounted Cumulative Gain at k (max possibile DCG at k).</p></li>
</ul>
<div class="highlight-bibtex notranslate"><div class="highlight"><pre><span></span><span class="nc">@article</span><span class="p">{</span><span class="nl">jarvelin2002cumulated</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">&quot;Cumulated gain-based evaluation of IR techniques&quot;</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">&quot;J{\&quot;a}rvelin, Kalervo and Kek{\&quot;a}l{\&quot;a}inen, Jaana&quot;</span><span class="p">,</span>
    <span class="na">journal</span><span class="p">=</span><span class="s">&quot;ACM Transactions on Information Systems (TOIS)&quot;</span><span class="p">,</span>
    <span class="na">volume</span><span class="p">=</span><span class="s">&quot;20&quot;</span><span class="p">,</span>
    <span class="na">number</span><span class="p">=</span><span class="s">&quot;4&quot;</span><span class="p">,</span>
    <span class="na">pages</span><span class="p">=</span><span class="s">&quot;422--446&quot;</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">&quot;2002&quot;</span><span class="p">,</span>
    <span class="na">publisher</span><span class="p">=</span><span class="s">&quot;ACM&quot;</span><span class="p">,</span>
    <span class="na">doi</span><span class="p">=</span><span class="s">&quot;10.1145/582415.582418&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>If y_true and y_pred are multi-dimensional, it computes the arithmetic mean of the Normalized Discounted Cumulative Gain scores.</p>
<p>Example:
&gt;&gt;&gt; y_true = np.array([[[12, 0.5], [25, 0.3]], [[11, 0.4], [2, 0.6]]])
&gt;&gt;&gt; y_pred = np.array([[12, 234, 25, 36, 32, 35], [12, 11, 25, 36, 2, 35]])
&gt;&gt;&gt; k = 5
&gt;&gt;&gt; ndcg(y_true, y_pred, k)
0.7525653965843032</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs and true relevance scores of _relevant_ documents.</p></li>
<li><p><strong>y_pred</strong> (<em>Numpy Array</em><em> or </em><em>List of Numpy Arrays</em><em> or </em><em>Numba Typed List</em>) – IDs of _retrieved_ documents.</p></li>
<li><p><strong>k</strong> (<em>Int</em>) – Number of results to consider.</p></li>
<li><p><strong>binary</strong> (<em>Boolean</em>) – If <cite>True</cite> it will not reorder y_true by relevance score during iDCG computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Normalized Discounted Cumulative Gain at k score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Float</p>
</dd>
</dl>
</dd></dl>

</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">metrics_eval</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">metrics_eval</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">metrics_eval package</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  <li><a href="modules.html">metrics_eval</a><ul>
      <li>Previous: <a href="modules.html" title="previous chapter">metrics_eval</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Elias Bassani.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.0.4</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/metrics_eval.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>